- No Code Tools for AI Agent : _zapier, n8n 
- with code Agentic AI framework : CrewAI , Langraph 
- In No Code tools we can build Agent within a minutes but the less control over the customization but In coding platform the main advantage is customization.
- In No code tools have fast development, but limited customization, Initial cost is low but as an when the usage increases the cost of the consumption also has increased.
- In other side, the coding platform like CrewAI, Langraph , we can do lost of customization, and we can also minimize the cost and optimize the cost using different ML and SW techniques


### What is Streaming: 
In LLM, streaming means model start sending tokens (words) as soon as they generated, instead of waitin for entire response to be ready before retruning it.

#### Why Streaming:
- faster response time low drop of rate.
- Mimics human like conversation ( build trust, keep alive, and keep user engage).
- Imporant of multi-model uis
- You can cancel mid of the way to save the tokens.
- You can interleaved UI updates, e.g., show "thinking..", show tools result.


### GenAI Ovservability:
- Ovservability is the ability to understand the system's internal fault by examining the external output like outputs, logs, matrix, and traces. In allows to diagnose the issue, understand the performance and improve the reliability of the system by analyzing the data generated by the system.
- LangSmith is a unified observability and evaluation platform where developer can debug , test and monitor AI app performance. 
- The following things are able to traced by the LangSmith - Input, output, all intermediate states output, latency, token usage, cost , error, Tags, Metadata and feedback. 


### LangChain Vs Langgraph Vs LangSmith

#### Notes : Building Effective Agent from Anthropic (SC : https://www.anthropic.com/engineering/building-effective-agents):

- What are Agents : Agent can be defined in several ways , some customers define agent as a fully autonomous system that operate independently over extended periods, using various tools to accomplish complex tasks. Others use the term to describe more prescriptive implementation that follow predefined workflows. At Anthropic, we categorize all these variations as Agentic system, but draw an important architectural distinction between workflows and agents. Worlflows are systems where LLM and tolls are orchestrated through predefined code paths.
    - Agents on the other hand, are systems where LLMS dynamically direct their own processes and tool usage, maintaining control over how they accomplish tasks. 
- When ( and when not) to use agents : Recommended only find the simple solution possible and increasing the complexity when needed. When more complexity is warranted, workflows offer predictability and consistency for well-defined tasks, whereas agents are the better option when flexibility and model-driven decision-making are needed at scale.
- When and how to use frameworks: There are many framework that make agentic systems easiear to implement, The Claude Agent SDK, Strands Agents SDK by AWS, Rivet a drag and drop GUP LLM workflow , Vellum another GUI tools for building agent. 

#### Building Block: The Augmented LLM :Th ebasic building block of agentic systems is an LLM enhanced with augmentations such as retrieval, tools and memory. Our current models can actively use these capabilities - generating their own search queries, selecting appropriate tools, and determining what information to retain. 
![The augmented LLM](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fd3083d3f40bb2b6f477901cc9a240738d3dd1371-2401x1000.png&w=3840&q=75)
#### Workflow : Prompt chaining : Prompt chaining decomposes a tasks into a sequence of steps, where each LLM call processes the output of the previous one. You can add programmatic checks on any intermediate steps to ensure that the process is still on track. This workflow is ideal for situations where the tasks can be easily and cleanly decompoesed into fixed subtasks.The main goal is to trade off latency for higher accuracy, by making each LLM call on easier task.Example , marketing copy, then translating into a different language. Another example , Write a outline of the document and checking them the outline meets certain criteria, then writing the document based on the outline.
![The prompt chaining workflow](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F7418719e3dab222dccb379b8879e1dc08ad34c78-2401x1000.png&w=3840&q=75)

#### Workflow : Routing 
Routing classifies an input and directs it to a specialized followup tasks. This workflow allows for seperation of concerns, and building more specialized prompts. Without this workflow , optimizing for one kind of input can hurt performance on other inputs. 
- Routing works well for complex tasks where there are distict categories that are better handled separately, and where classification can be handled accurately.
- Exmaple : Directing different types of customer service queries ( general questions, refund requests, technical support) into different downstream processes , prompts and tools. 
![The routing workflow](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F5c0c0e9fe4def0b584c04d37849941da55e5e71c-2401x1000.png&w=3840&q=75)

#### Workflow : Parallelization 
LLMs can sometimes work simultaneously on a tasks and have their output aggregated programmatically. Sectioning: Breaking a task into independent subtasks run in parallel. Voting: Running the same tasks multiple times to get diverse outputs.
- Paralleniztion is effective when the divided subtasks can be parallelized for speed, or when multiple perspectives or attempts are needed for higher confidence results. For complex tasks with multiple considerations,LLM generally perform better when each consideration is handled by a separate LLM call, alowing focused attention on each specific aspect.
- Sectioning : - Implementing guardrails where one model instance processes user queries while another screens them for inappropriate content or requests. This tends to perform better than having the same LLM call handle both guardrails and the core response. 
    - Automating vals for evaluating LLM performance, where each LLM call evaluates a different aspect of the model's performance on a given prompt. 
- Voting : Reviewing a piece of code for vulnerabilities, where several different prompts review and flag the code if they find a problem. Another one , Evaluating whether a given piece of content is inappropriate, with multiple prompts evaluating different aspects or requiring different vote thresholds to balance false positives and negatives.
![The parallelization workflow](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F406bb032ca007fd1624f261af717d70e6ca86286-2401x1000.png&w=3840&q=75)

#### Workflow : Orchestrator - workers :
In the orchestrator-workers workflow, a central LLM dynamically breaks down tasks, delegates them to worker LLMs, and synthesizes their results. 
- This workflow is well-suited for complex tasks where you can't predict the subtasks needed.( in coding, for example, the number of files that need to be changed and the nature of the change in each file likely depend on the task). Whereas it's topographically similar , the key difference from parallelization is its flexibility - subtasks aren't pre-defined, but determined by the orchestrator based on the specific input.
- For example , Search tasks that involve gathering and analyzing information from multiple sources for possible relevant information. 
![The orchestrator-workers workflow](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F8985fc683fae4780fb34eab1365ab78c7e51bc8e-2401x1000.png&w=3840&q=75)


#### Workflow : Evaluator-optimizer 
IN the evaluator-optimizer workflow, one LLM call generates the response while another proves evaluation and feedback in a loop.This workflow effective when we have clear evaluation criteria and when interactive refinement proves measurable value. 
- Example, Literary translation where there are nuances that the translator LLM might not capture initalyly, but where an evaluator LLM can provide useful critiques. 
- Another Example, complex search tasks that require multiple rounds of searching and analysis to gather comprehensive information, where the evaluator decides whether further searches are warranted. 
![The evaluator-optimizer workflow](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F14f51e6406ccb29e695da48b17017e899a6119c7-2401x1000.png&w=3840&q=75)

#### Agent :
Agents are emerging in production as LLMs mature in key capabilities—understanding complex inputs, engaging in reasoning and planning, using tools reliably, and recovering from errors. Agents begin their work with either a command from, or interactive discussion with, the human user. Once the task is clear, agents plan and operate independently, potentially returning to the human for further information or judgement. During execution, it's crucial for the agents to gain “ground truth” from the environment at each step (such as tool call results or code execution) to assess its progress. Agents can then pause for human feedback at checkpoints or when encountering blockers. The task often terminates upon completion, but it’s also common to include stopping conditions (such as a maximum number of iterations) to maintain control. 

Agents can handle sophisticated tasks, but their implementation is often straightforward. They are typically just LLMs using tools based on environmental feedback in a loop. It is therefore crucial to design toolsets and their documentation clearly and thoughtfully. We expand on best practices for tool development in Appendix 2 ("Prompt Engineering your Tools").


